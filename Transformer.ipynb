{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "ab9def6ddb3013bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "# Download tiny shakespeare\n",
    "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# Encode dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Dataset size: {len(data):,} characters\")\n",
    "print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}\")"
   ],
   "id": "43a69a34cb11be19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y"
   ],
   "id": "21eaf0466b0de49c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Project and split into Q, K, V\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply causal mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out = self.attn(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_out = self.ff(self.norm2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Create causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "\n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Output\n",
    "        x = self.norm(x)\n",
    "        logits = self.fc_out(x)\n",
    "\n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if it exceeds max_len\n",
    "            idx_cond = idx if idx.size(1) <= self.max_len else idx[:, -self.max_len:]\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Optional top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ],
   "id": "fd5f5a2736fd2879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Model hyperparameters\n",
    "block_size = 128      # Context length\n",
    "d_model = 256         # Embedding dimension\n",
    "num_heads = 8         # Number of attention heads\n",
    "num_layers = 6        # Number of transformer blocks\n",
    "d_ff = 1024          # Feed-forward dimension\n",
    "dropout = 0.2\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CharDataset(train_data, block_size)\n",
    "val_dataset = CharDataset(val_data, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_len=block_size,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "id": "47c37b787f248615"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = {'train': [], 'val': []}\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= eval_iters:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            losses[split].append(loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return {k: np.mean(v) for k, v in losses.items()}\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for iter_num in range(max_iters):\n",
    "        # Get batch\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            break  # One batch per iteration\n",
    "\n",
    "        # Evaluate\n",
    "        if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"Step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")"
   ],
   "id": "391e73ee7c081a7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# GENERATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_text(prompt, max_new_tokens=500, temperature=0.8, top_k=40):\n",
    "    \"\"\"Generate text from a prompt\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt\n",
    "    if prompt:\n",
    "        context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
    "    else:\n",
    "        context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "    # Generate\n",
    "    generated = model.generate(context, max_new_tokens, temperature, top_k)\n",
    "    output = decode(generated[0].tolist())\n",
    "\n",
    "    return output"
   ],
   "id": "fa9c68862a82ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "train()"
   ],
   "id": "cb83cceaeed66188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# TEST GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Generation\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be,\",\n",
    "    \"What light through yonder window\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    output = generate_text(prompt, max_new_tokens=200, temperature=0.8, top_k=40)\n",
    "    print(output)\n",
    "    print(\"\\n\")"
   ],
   "id": "9ebad080cd6307d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE CHAT FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def chat():\n",
    "    \"\"\"Interactive chat with the model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GPT Chat Mode (type 'quit' to exit)\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = generate_text(prompt, max_new_tokens=300, temperature=0.8, top_k=40)\n",
    "        print(f\"\\nGPT: {response}\\n\")\n",
    "\n",
    "# Uncomment to start chatting:\n",
    "chat()"
   ],
   "id": "a305682ec082c897"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
